\section{Parameter optimization}
In the previous section it has been shown that  the random generated $r$ coordinates that statistically follow the satellite profile (eq. 6) are distributed as a poissonian around the satellite profile. However, these $r$ coordinates were randomly generated given certain $a$, $b$, and $c$ values. If the $r$ coordinates are measured and given instead of the values of $a$, $b$, and $c$, it is also possible to compute $a$, $b$, and $c$ from this data.Thus, the whole process is reversed. \\
\\
Given are 5 files that contain the spherical coordinates of haloes in a certain mass bin. Each halo contains a variable number of satellites. This section will have a closer look on the given data and tries to maximize the log-likelihood to find the most optimal parameters $a$, $b$, and $c$, given the data. Also, an attempt is made to fit these found $a$, $b$, and $c$ values against the mass.
\subsection{Maximum Likelihood}
Since the $r$ coordinates are distributed as a poissonian around the number density profile, the likelihood that has to be minimized is:
\begin{equation*}
\mathcal{L} = \prod_{i=0}^{N-1} \frac{\mu(x_i|\textbf{p})^{y_i}e^{-\mu(x_i|\textbf{p})}}{y_i!}
\end{equation*}
Here $x_i$ is the $r$ coordinate of galaxy $i$ in the data file, $\textbf{p}$ are the parameters to be optimized $(a,b,c)$, $\mu$ is the average number of satellites at radius $x_i$, and $y_i$ is the number of galaxies in the bin that $x_i$ belongs. In the previous section it has been given that the average number of satellites at radius $x$ is given by $N(x) = n(x)4\pi x^2$, and thus $\mu(x_i|\textbf{p}) = N(x_i|\textbf{p})$.
Taking the negative log of the likelihood gives,
\begin{equation*}
- \ln(\mathcal{L}) = -\sum_{i=0}^{n-1} \left[y_i\ln(N(x_i|\textbf{p})) - N(x_i|\textbf{p}) - \ln(y_i!)\right]
\end{equation*}
Here $n$ is the number of bins used to bin the data.
Binning the data could result in some empty bins. Taking the logarithm of zero results in \texttt{nans} and/or minus inifinities. To avoid this problem it is possible to make the whole expression, shown above, continuous. This can be done by making the bins so small that each bin contains either 0 or 1 galaxy. The last term in the log-likelihood will therefore always be equal to zero, since $1! = 0! = 1$ and $\ln(1) = 0$. The middle term can be evaluated as an integral because the infinitesimally small bins make the function continuous and thus for each $x$ radius in the given interval, the $N(x)$ is being determined. And the first term that contains the factor $y_i$ will either be equal to zero or equal to one, thus in the end the log-likelihood reduces to,
\begin{equation}
-\ln(\mathcal{L}) = -\sum_{i=0}^{n-1} \left[\ln(N(x|\textbf{p}))\right] + \int N(x_i|\textbf{p})\mathrm{d}x
\end{equation}
where $n$ is now the number of given data points in the file.
Looking closer at the integral, one can see that it is actually exactly the same as the integral in eq. 7. Therefore, the integral can also be written as $\int N(x_i|\textbf{b})\mathrm{d}x = \langle N_{sat} \rangle$. This is the average total number of satellites and is independent of the parameters $\mathbf{p}$. Given a data file containing $k$ haloes and $q$ satellites, the average total number of satellites is $\langle N_{sat} \rangle = q/k$. However, it is desirable to obtain the same $a$, $b$, and $c$ values for each halo in the same data file. To obtain these values, a rough assumption has to be made that the number density profiles of each halo is the same for a given mass $m$. If this is true, then the distribution of satellites can be assumed to be the same for each halo as well. To make things simpler, the halo distributions can then be added to each other, resulting in one giant halo. The average total number of satellites is then $\langle N_{sat} \rangle = q$, since $k=1$. The average number of satellites at radius $x_i$ depends also on the normalization factor $A$. This normalization factor depends, as well, on $a$, $b$, and $c$, as stated in the previous section. Since the trilinear interpolation method needs less computations than the Romberg integration method, it is more efficient to use trilinear interpolation to find the normalization factor $A$, given $(a,b,c)$. Now all the parameters are defined and the log-likelihood can be minimized. This minimization can either be done by taking the derivative of the log-likelihood to the parameters and equation it to zero, or by simply minimizing the log-likelihood function with the use of a multidimensional minimization algorithm. In this paper, the latter has been used because the derivative of this log-likelihood function is not easy to compute. The algorithm that has been used to minimize the log-likelihood is the downhill simplex algorithm. The reason for this is because this method does not require any derivative, and thus makes the problem much easier to solve.\footnote{Methods using derivatives are much more efficient to use, so if I had more time I would definitely try this method first.} A disadvantage is that this algorithm is less efficient than algorithms that do use derivatives. In addition, the downhill simplex algorithm is very sensitive to initial guesses. Therefore, a rough estimate has been made on the minimum which is put into the downhill simplex algorithm.\footnote{These estimates are found by looking at the average number of satellites at radius $x_i$ plot, together with the histogram of the $r$ coordinates of the super halo (where the bins are divided by the widths too make sure it is normalized correctly). If $N(x_i|\textbf{p})$ seemed to show some similarity with the histogram, the rough estimation was good enough.} \\
The errors on the parameters can not be found directly. However, it is possible to find the relative errors. This relative error is based on a 'steepness' test of the log-likelihood function given the data. If, for example, for one data file the log-likelihood is much steeper around the found minimum, then it is more likely that this found minimum is more nearby the actual minimum than a less steep log-likelihood function. There are several methods for finding this error. In the code below the following method is used. Assuming that the found minimum is nearby the actual minimum, a cube is made surrounding the found minimum. The negative log-likelihood values of the vertices of this cube. These negative log-likelihoods are then subtracted from the negative log-likelihood at the found minimum. These differences between vertices of the cube and found minimum are then added to each other, and finally it is normalized by dividing it by the negative log-likelihood of the minimum. If the negative log-likelihood function is steep than this difference will be greater than if the negative log-likelihood function is less steep. Therefore, this computed normalized difference is a measure of steepness of the function.\footnote{The actual method used to find the relative errors is in reality a bit different and is the reversed of this process, however the method used here is a bit easier.}\\
The code that has been used is shown below.
\lstinputlisting{Q3a.py}
\lstinputlisting[firstline=712,lastline=889]{functions.py}
The code outputs the $a$, $b$, and $c$ values belonging to each mass.

\lstinputlisting{textfiles/likelihood.txt}